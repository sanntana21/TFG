{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanntana21/TFG/blob/main/matrix_time_series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHYH674PSqkP"
      },
      "source": [
        "# **PREDICCIONES PARA TODA LA POBLACION SIN AGREGACION**"
      ],
      "id": "UHYH674PSqkP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import seaborn as sbn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import plotly.express as px\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def set_output_precision(decimals):\n",
        "  \"\"\"\n",
        "  format the output of the all the data structures\n",
        "  with an specific number of decimals\n",
        "  \"\"\"\n",
        "  np.set_printoptions(precision=decimals)\n",
        "  into='{'+':.{}f'.format(decimals)+'}'\n",
        "  pd.options.display.float_format = into.format\n",
        "  torch.set_printoptions(precision=decimals)\n",
        "  pass\n",
        "\n",
        "def plot_ts(df,dfx=\"Minute\",dfy=\"METS\",_title=\"DF minute x Mets\"):\n",
        "  if not isinstance(df,pd.DataFrame):\n",
        "    df = pd.DataFrame({'METS': df, 'Minute': range(len(df))})\n",
        "\n",
        "  plt.figure()\n",
        "  fig = px.line(df, x = dfx, y = dfy , title = _title)\n",
        "  fig.update_xaxes(\n",
        "      rangeslider_visible = True,\n",
        "      rangeselector = dict(\n",
        "          buttons = list([\n",
        "              dict(count=1,label=\"1y\",step=\"year\",stepmode=\"backward\"),\n",
        "              dict(count=2,label=\"2y\",step=\"year\",stepmode=\"backward\"),\n",
        "              dict(count=3,label=\"3y\",step=\"year\",stepmode=\"backward\"),\n",
        "              dict(step=\"all\")\n",
        "          ])\n",
        "      )\n",
        "\n",
        "  )\n",
        "  fig.show()\n",
        "\n",
        "def plot_predictions_vs_real(predictions, reals):\n",
        "  df = pd.DataFrame()\n",
        "  number_of_points = len(predictions)\n",
        "  df[\"time\"] = range(0,number_of_points)\n",
        "  df[\"participant\"] = \"prediction\"\n",
        "  df[\"value\"] = predictions\n",
        "  for i in range(0,number_of_points):\n",
        "    df.loc[number_of_points+i] = [i,\"real\",reals[i]]\n",
        "\n",
        "  plt.figure(1)\n",
        "  fig = px.line(df, x = \"time\", y = \"value\" , title = \"predictions vs reals\" , color = \"participant\")\n",
        "  fig.update_xaxes(\n",
        "        rangeslider_visible = True,\n",
        "        rangeselector = dict(\n",
        "            buttons = list([\n",
        "                dict(count=1,label=\"1y\",step=\"year\",stepmode=\"backward\"),\n",
        "                dict(count=2,label=\"2y\",step=\"year\",stepmode=\"backward\"),\n",
        "                dict(count=3,label=\"3y\",step=\"year\",stepmode=\"backward\"),\n",
        "                dict(step=\"all\")\n",
        "            ])\n",
        "        )\n",
        "\n",
        "    )\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "EAl4ruc6Dtmj"
      },
      "id": "EAl4ruc6Dtmj"
    },
    {
      "cell_type": "code",
      "source": [
        "INDEXS = [{\"train\":[{\"start\":{\"day\":2,\"hour\":0,\"minute\":0},\n",
        "                     \"end\":{\"day\":20,\"hour\":22,\"minute\":0}}\n",
        "                    ],\n",
        "           \"validation\":[{\"start\":{\"day\":22,\"hour\":0,\"minute\":0},\n",
        "                     \"end\":{\"day\":24,\"hour\":22,\"minute\":0}}\n",
        "                    ],\n",
        "           \"test\":[{\"start\":{\"day\":26,\"hour\":0,\"minute\":0},\n",
        "                     \"end\":{\"day\":27,\"hour\":22,\"minute\":0}}\n",
        "                    ]}\n",
        "          ]"
      ],
      "metadata": {
        "id": "8wZw9EjwD5Se"
      },
      "id": "8wZw9EjwD5Se",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x-vPqs_LjtB"
      },
      "outputs": [],
      "source": [
        "READ_LOCAL_DATA = False\n",
        "COMPUTED_OPTION = 0\n",
        "TEST_SIZE = 0.2\n",
        "VALIDATION_SIZE = 0.15\n",
        "SAVE_RESULTS = True\n",
        "LOW_DATA = True\n",
        "SPLIT = 0\n",
        "MULTI_STEP_FORECAST = False"
      ],
      "id": "3x-vPqs_LjtB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UBiboWvo0TI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a1a50c-4b94-4ac3-fea2-e6a33e68eb22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if READ_LOCAL_DATA:\n",
        "  PATH = \"Resources/Individual/\"\n",
        "else:\n",
        "    #  We start by getting access to the drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PATH = \"/content/drive/MyDrive/TFG/Resources/Individual/\"\n",
        "\n",
        "if LOW_DATA:\n",
        "    PATH += \"LowData/\""
      ],
      "id": "2UBiboWvo0TI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wx-eUkHY7S6",
        "outputId": "48394049-af19-4634-bf6a-3ed2ff4a6de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\t SHAPES\u001b[0m\n",
            "(25, 40200, 1440)\n",
            "(25, 40200, 120)\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "documents = ['minuteY','hourY','dayY']\n",
        "file = PATH+\"minuteX.pkl.gz\"\n",
        "dataX = np.array(pickle.load(gzip.open(file, 'rb')),np.float32)\n",
        "\n",
        "file = PATH+documents[COMPUTED_OPTION]+\".pkl.gz\"\n",
        "dataY = np.array(pickle.load(gzip.open(file, 'rb')),np.float32)\n",
        "\n",
        "NUMBER_OF_PARTICIPANTS = dataX.shape[0]\n",
        "print('\\033[1m' + \"\\t SHAPES\" + '\\033[0m')\n",
        "print(dataX.shape)\n",
        "print(dataY.shape)\n",
        "dataX = tf.convert_to_tensor(dataX)\n",
        "dataY = tf.convert_to_tensor(dataY)"
      ],
      "id": "_Wx-eUkHY7S6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "if MULTI_STEP_FORECAST:\n",
        "  if COMPUTED_OPTION == 0:\n",
        "      dataX_nuevo = dataX\n",
        "      JUMP = 1\n",
        "  elif COMPUTED_OPTION == 1:\n",
        "      JUMP = 60\n",
        "      dataX_nuevo = np.ones(shape=(NUMBER_OF_PARTICIPANTS,dataX.shape[1],24))\n",
        "      for k in range(0,NUMBER_OF_PARTICIPANTS):\n",
        "          for i in range(0,dataX.shape[1]):\n",
        "              for j in range(0,24):\n",
        "                  dataX_nuevo[k,i,j] = np.sum(dataX[k,i,60*j:60*(j+1)])\n",
        "  else:\n",
        "      JUMP = 1440\n",
        "      dataX_nuevo = np.ones(shape=(dataX.shape[0],1))\n",
        "      for i in range(0,dataX.shape[0]):\n",
        "          dataX_nuevo[i,:] = np.sum(dataX[i,:])\n"
      ],
      "metadata": {
        "id": "STmV2x6vDtml"
      },
      "id": "STmV2x6vDtml"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2182a00"
      },
      "outputs": [],
      "source": [
        "#We split a test set for testing\n",
        "# train_test_split(dataX, dataY, test_size=TEST_SIZE)\n",
        "\n",
        "def calculate_index(time):\n",
        "  minute_index = time[\"day\"] * 1440 + time[\"hour\"]*60 + time[\"minute\"]\n",
        "  return minute_index\n",
        "\n",
        "def get_split(dataX,dataY,index):\n",
        "  start = calculate_index(index[0][\"start\"])\n",
        "  end = calculate_index(index[0][\"end\"])\n",
        "  X_split = dataX[:,start:end,:]\n",
        "  y_split = dataY[:,start:end,:]\n",
        "  if len(index) > 1:\n",
        "    for i in range(1,index):\n",
        "      start = calculate_index(index[i][\"start\"])\n",
        "      end = calculate_index(index[i][\"end\"])\n",
        "      X_split = np.concatenate(X_split,dataX[:,start:end,:])\n",
        "      y_split = np.concatenate(y_split,dataY[:,start:end,:])\n",
        "  return X_split,y_split\n",
        "\n",
        "def train_test_validation_split(dataX,dataY,indexs):\n",
        "  X_train,y_train = get_split(dataX,dataY,indexs[\"train\"])\n",
        "  X_validation,y_validation = get_split(dataX,dataY,indexs[\"validation\"])\n",
        "  X_test,y_test = get_split(dataX,dataY,indexs[\"test\"])\n",
        "  return X_train,y_train,X_validation,y_validation,X_test,y_test\n",
        "\n",
        "\n",
        "X_train,y_train,X_validation,y_validation,X_test,y_test = train_test_validation_split(dataX,dataY,INDEXS[SPLIT])\n",
        "del dataX\n",
        "del dataY\n",
        "print(\"Examples for training\\n\",\"X:\",X_train.shape,\"y:\",y_train.shape)\n",
        "print(\"Examples for validation\\n\",\"X:\",X_validation.shape,\"y:\",y_validation.shape)\n",
        "print(\"Examples for test\\n\",\"X:\",X_test.shape,\"y:\",y_test.shape)\n",
        "\n",
        "#Quitamos en este caso la primera dimension\n",
        "X_train,y_train,X_validation,y_validation,X_test,y_test = [\n",
        "  tf.convert_to_tensor(i) for i in [X_train,y_train,X_validation,y_validation,X_test,y_test]\n",
        "]"
      ],
      "id": "c2182a00"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4V6iWDmyjDq"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "# Setup dataset hyperparameters\n",
        "HORIZON = y_test.shape[2]\n",
        "WINDOW_SIZE = X_train.shape[2]\n",
        "\n",
        "# Let's build an LSTM model with the Functional API\n",
        "inputs = layers.Input(shape=(NUMBER_OF_PARTICIPANTS ,WINDOW_SIZE))\n",
        "# x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM\n",
        "# print(x.shape)\n",
        "# x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\n",
        "x = layers.LSTM(128,return_sequences=True, activation=\"relu\")(inputs) # using the tanh loss function results in a massive error\n",
        "# print(x.shape)\n",
        "# Add another optional dense layer (you could add more of these to see if they improve model performance)\n",
        "# x = layers.Dense(32, activation=\"relu\")(x)\n",
        "output = layers.Dense(HORIZON)(x)\n",
        "model_LSTM = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")\n",
        "\n",
        "model_LSTM.summary()"
      ],
      "id": "C4V6iWDmyjDq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Compile model\n",
        "model_LSTM.compile(loss=\"mae\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "             metrics=[\"mae\"])"
      ],
      "metadata": {
        "id": "_2z94NSDDtmn"
      },
      "id": "_2z94NSDDtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554\n",
        "model_LSTM.fit(X_train,\n",
        "            y_train,\n",
        "            epochs=3,\n",
        "            verbose=1,\n",
        "            batch_size=128,\n",
        "            validation_data=(X_validation, y_validation))"
      ],
      "metadata": {
        "id": "qOsSsddWDtmn"
      },
      "id": "qOsSsddWDtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlZrCDyy0yPY"
      },
      "outputs": [],
      "source": [
        "def make_preds(model, input_data):\n",
        "  \"\"\"\n",
        "  Uses model to make predictions on input_data.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model: trained model\n",
        "  input_data: windowed input data (same kind of data model was trained on)\n",
        "\n",
        "  Returns model predictions on input_data.\n",
        "  \"\"\"\n",
        "  forecast = model.predict(input_data,verbose=2)\n",
        "  return tf.squeeze(forecast) # return 1D array of predictions\n",
        "\n",
        "predictions = make_preds(model_LSTM, X_test)"
      ],
      "id": "WlZrCDyy0yPY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "if SAVE_RESULTS:\n",
        "    file_path = 'Resources/Resultados/Matrix/'\n",
        "\n",
        "    if LOW_DATA:\n",
        "        file_path  += \"LowData/\"\n",
        "\n",
        "    documents = ['minuteY','hourY','dayY']\n",
        "    file = file_path+documents[COMPUTED_OPTION]+\"-predictions\"+\".pkl.gz\"\n",
        "    pickle.dump(predictions, gzip.open(file, 'wb'))\n",
        "    file = file_path+documents[COMPUTED_OPTION]+\"-test\"+\".pkl.gz\"\n",
        "    pickle.dump(y_test, gzip.open(file, 'wb'))\n",
        "    file = file_path+documents[COMPUTED_OPTION]+\"-X\"+\".pkl.gz\"\n",
        "    pickle.dump(X_test, gzip.open(file, 'wb'))"
      ],
      "metadata": {
        "id": "bpawkRp9Dtmn"
      },
      "id": "bpawkRp9Dtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "day_before = np.array(X_test[0:1,:,:],dtype=np.float32)\n",
        "predicted_values_day_one = np.ones(shape=(WINDOW_SIZE,NUMBER_OF_PARTICIPANTS,1),dtype=np.float32)\n",
        "print(predicted_values_day_one.shape)\n",
        "print(day_before.shape)\n",
        "for i in range(0,WINDOW_SIZE):\n",
        "    prediction = make_preds(model_LSTM, day_before)\n",
        "    day_before[:,:,0:-1] = day_before[:,:,1:]\n",
        "    day_before[:,:,-1] = prediction[:]\n",
        "    predicted_values_day_one[i,:,0] = prediction[:]"
      ],
      "metadata": {
        "id": "C-ZlfvH7Dtmn"
      },
      "id": "C-ZlfvH7Dtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "SECOND_DAY = X_test.shape[2]\n",
        "day_before = np.array(X_test[SECOND_DAY:SECOND_DAY+1,:,:],dtype=np.float32)\n",
        "predicted_values_day_two = np.ones(shape=(WINDOW_SIZE,NUMBER_OF_PARTICIPANTS,1),dtype=np.float32)\n",
        "for i in range(0,WINDOW_SIZE):\n",
        "    prediction = make_preds(model_LSTM, day_before)\n",
        "    day_before[:,:,0:-1] = day_before[:,:,1:]\n",
        "    day_before[:,:,-1] = prediction[:]\n",
        "    predicted_values_day_two[i,:,0] = prediction[:]"
      ],
      "metadata": {
        "id": "QwUn28SFDtmn"
      },
      "id": "QwUn28SFDtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "poblational_predicted_values_day_one = np.sum(predicted_values_day_one,axis=1)\n",
        "poblational_predicted_values_day_two = np.sum(predicted_values_day_two,axis=1)\n",
        "poblational_y_test = np.sum(y_test,axis=1)"
      ],
      "metadata": {
        "id": "2h1TxX17Dtmn"
      },
      "id": "2h1TxX17Dtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"Predecidos : \" + str(int(np.sum(poblational_predicted_values_day_one[:,0]))))\n",
        "print(\"Reales : \" + str(int(np.sum(poblational_y_test[0:WINDOW_SIZE,0]))))\n",
        "if COMPUTED_OPTION < 2:\n",
        "    plot_predictions_vs_real(poblational_predicted_values_day_one[:,0],poblational_y_test[0:WINDOW_SIZE,0])"
      ],
      "metadata": {
        "id": "LzbuFhTuDtmn"
      },
      "id": "LzbuFhTuDtmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"Predecidos : \" + str(int(np.sum(poblational_predicted_values_day_two[:,0]))))\n",
        "print(\"Reales : \" + str(int(np.sum(poblational_y_test[SECOND_DAY:,0]))))\n",
        "if COMPUTED_OPTION < 2:\n",
        "    plot_predictions_vs_real(poblational_predicted_values_day_two[:100,0],poblational_y_test[SECOND_DAY:SECOND_DAY+100,0])"
      ],
      "metadata": {
        "id": "o3ELCJ6-Dtmo"
      },
      "id": "o3ELCJ6-Dtmo"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}