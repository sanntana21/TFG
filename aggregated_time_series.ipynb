{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHYH674PSqkP"
   },
   "source": [
    "# **PREDICCIONES AGREGADAS**"
   ],
   "id": "UHYH674PSqkP"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sbn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "\n",
    "def set_output_precision(decimals):\n",
    "  \"\"\"\n",
    "  format the output of the all the data structures\n",
    "  with an specific number of decimals\n",
    "  \"\"\"\n",
    "  np.set_printoptions(precision=decimals)\n",
    "  into='{'+':.{}f'.format(decimals)+'}'\n",
    "  pd.options.display.float_format = into.format\n",
    "  pass\n",
    "\n",
    "def plot_ts(df,dfx=\"Minute\",dfy=\"METS\",_title=\"DF minute x Mets\"):\n",
    "  if not isinstance(df,pd.DataFrame):\n",
    "    df = pd.DataFrame({'METS': df, 'Minute': range(len(df))})\n",
    "\n",
    "  plt.figure()\n",
    "  fig = px.line(df, x = dfx, y = dfy , title = _title)\n",
    "  fig.update_xaxes(\n",
    "      rangeslider_visible = True,\n",
    "      rangeselector = dict(\n",
    "          buttons = list([\n",
    "              dict(count=1,label=\"1y\",step=\"year\",stepmode=\"backward\"),\n",
    "              dict(count=2,label=\"2y\",step=\"year\",stepmode=\"backward\"),\n",
    "              dict(count=3,label=\"3y\",step=\"year\",stepmode=\"backward\"),\n",
    "              dict(step=\"all\")\n",
    "          ])\n",
    "      )\n",
    "\n",
    "  )\n",
    "  fig.show()\n",
    "\n",
    "def plot_predictions_vs_real(predictions, reals):\n",
    "  df = pd.DataFrame()\n",
    "  number_of_points = len(predictions)\n",
    "  df[\"time\"] = range(0,number_of_points)\n",
    "  df[\"participant\"] = \"prediction\"\n",
    "  df[\"value\"] = predictions\n",
    "  for i in range(0,number_of_points):\n",
    "    df.loc[number_of_points+i] = [i,\"real\",reals[i]]\n",
    "\n",
    "  plt.figure(1)\n",
    "  fig = px.line(df, x = \"time\", y = \"value\" , title = \"predictions vs reals\" , color = \"participant\")\n",
    "  fig.update_xaxes(\n",
    "        rangeslider_visible = True,\n",
    "        rangeselector = dict(\n",
    "            buttons = list([\n",
    "                dict(count=1,label=\"1y\",step=\"year\",stepmode=\"backward\"),\n",
    "                dict(count=2,label=\"2y\",step=\"year\",stepmode=\"backward\"),\n",
    "                dict(count=3,label=\"3y\",step=\"year\",stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    )\n",
    "  fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3x-vPqs_LjtB"
   },
   "outputs": [],
   "source": [
    "READ_LOCAL_DATA = True\n",
    "COMPUTED_OPTION = 0\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.15\n",
    "SAVE_RESULTS = False\n",
    "LOW_DATA = True\n",
    "SPLIT_INTO_TWO_DAYS = False\n",
    "MULTI_STEP_FORECAST = False\n",
    "np.random.seed(42)"
   ],
   "id": "3x-vPqs_LjtB"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2UBiboWvo0TI"
   },
   "outputs": [],
   "source": [
    "if READ_LOCAL_DATA:\n",
    "  PATH = \"Resources/Agregado/\"\n",
    "else:\n",
    "    #  We start by getting access to the drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PATH = \"/content/drive/MyDrive/TFG/Resources/Agregado/\"\n",
    "\n",
    "\n",
    "if LOW_DATA:\n",
    "    PATH += \"LowData/\""
   ],
   "id": "2UBiboWvo0TI"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Wx-eUkHY7S6",
    "outputId": "0af8d1fc-e14f-46d4-8db5-8f643d562ff5"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "documents = ['minuteY','hourY','dayY']\n",
    "# with open(PATH+\"minuteX\"+\".pkl\", 'rb') as file:\n",
    "#     dataX = np.array(pickle.load(file),np.float32)\n",
    "file = PATH+\"minuteX.pkl.gz\"\n",
    "dataX = np.array(pickle.load(gzip.open(file, 'rb')),np.float32)\n",
    "\n",
    "file = PATH+documents[COMPUTED_OPTION]+\".pkl.gz\"\n",
    "dataY = np.array(pickle.load(gzip.open(file, 'rb')),np.float32)"
   ],
   "id": "_Wx-eUkHY7S6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if MULTI_STEP_FORECAST:\n",
    "    PREDICTED_HORIZON = 1\n",
    "    if COMPUTED_OPTION == 0:\n",
    "        dataX_nuevo = dataX\n",
    "        JUMP = 1\n",
    "    elif COMPUTED_OPTION == 1:\n",
    "        JUMP = 60\n",
    "        dataX_nuevo = np.ones(shape=(dataX.shape[0],24))\n",
    "        for i in range(0,dataX.shape[0]):\n",
    "            for j in range(0,24):\n",
    "                dataX_nuevo[i,j] = np.sum(dataX[i,60*j:60*(j+1)])\n",
    "    else:\n",
    "        JUMP = 1440\n",
    "        dataX_nuevo = np.ones(shape=(dataX.shape[0],1))\n",
    "        for i in range(0,dataX.shape[0]):\n",
    "            dataX_nuevo[i,:] = np.sum(dataX[i,:])\n",
    "else:\n",
    "    JUMP = 1\n",
    "    if COMPUTED_OPTION == 0:\n",
    "        PREDICTED_HORIZON = 120\n",
    "    elif COMPUTED_OPTION == 1:\n",
    "        PREDICTED_HORIZON = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2182a00",
    "outputId": "e280fc1d-d52a-49b3-f3b8-b74ee23fee3f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m     X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m dataX_nuevo[:LAST_TWO_DAYS_INDEX,:],dataX_nuevo[LAST_TWO_DAYS_INDEX::JUMP,:],\\\n\u001B[0;32m      6\u001B[0m         dataY[:LAST_TWO_DAYS_INDEX:,\u001B[38;5;241m0\u001B[39m:PREDICTED_HORIZON],dataY[LAST_TWO_DAYS_INDEX::JUMP,\u001B[38;5;241m0\u001B[39m:PREDICTED_HORIZON]\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m----> 8\u001B[0m     X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTEST_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                                        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     y_test \u001B[38;5;241m=\u001B[39m y_test[:,\u001B[38;5;241m0\u001B[39m:PREDICTED_HORIZON]\n\u001B[0;32m     12\u001B[0m     y_train \u001B[38;5;241m=\u001B[39m y_train[:,\u001B[38;5;241m0\u001B[39m:PREDICTED_HORIZON]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2559\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[0;32m   2561\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m-> 2562\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_shuffle_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_test_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\n\u001B[0;32m   2564\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2566\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m   2567\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stratify \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001B[0m, in \u001B[0;36m_validate_shuffle_split\u001B[1;34m(n_samples, test_size, train_size, default_test_size)\u001B[0m\n\u001B[0;32m   2233\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(n_train), \u001B[38;5;28mint\u001B[39m(n_test)\n\u001B[0;32m   2235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWith n_samples=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, test_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and train_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2238\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresulting train set will be empty. Adjust any of the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2239\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maforementioned parameters.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_samples, test_size, train_size)\n\u001B[0;32m   2240\u001B[0m     )\n\u001B[0;32m   2242\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m n_train, n_test\n",
      "\u001B[1;31mValueError\u001B[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#We split a test set for testing\n",
    "# train_test_split(dataX, dataY, test_size=TEST_SIZE)\n",
    "if SPLIT_INTO_TWO_DAYS:\n",
    "    LAST_TWO_DAYS_INDEX = -1440*2\n",
    "    X_train, X_test, y_train, y_test = dataX_nuevo[:LAST_TWO_DAYS_INDEX,:],dataX_nuevo[LAST_TWO_DAYS_INDEX::JUMP,:],\\\n",
    "        dataY[:LAST_TWO_DAYS_INDEX:,0:PREDICTED_HORIZON],dataY[LAST_TWO_DAYS_INDEX::JUMP,0:PREDICTED_HORIZON]\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=TEST_SIZE,shuffle=True,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    y_test = y_test[:,0:PREDICTED_HORIZON]\n",
    "    y_train = y_train[:,0:PREDICTED_HORIZON]\n",
    "\n",
    "print(\"Examples for training\\n\",\"X:\",X_train.shape,\"y:\",y_train.shape)\n",
    "print(\"Examples for test\\n\",\"X:\",X_test.shape,\"y:\",y_test.shape)\n",
    "VALIDATION_INDEX = int(len(X_train)*VALIDATION_SIZE)\n",
    "print(\"Examples for validation: \\n\", VALIDATION_INDEX)"
   ],
   "id": "c2182a00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4V6iWDmyjDq",
    "outputId": "2cc487f9-b084-4f58-f441-b83014879d6a"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "# Setup dataset hyperparameters\n",
    "HORIZON = y_test.shape[1]\n",
    "WINDOW_SIZE = X_test.shape[1]\n",
    "\n",
    "# Let's build an LSTM model with the Functional API\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM\n",
    "# print(x.shape)\n",
    "# x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\n",
    "x = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error\n",
    "# print(x.shape)\n",
    "# Add another optional dense layer (you could add more of these to see if they improve model performance)\n",
    "# x = layers.Dense(32, activation=\"relu\")(x)\n",
    "output = layers.Dense(HORIZON)(x)\n",
    "model_LSTM = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")\n",
    "\n",
    "model_LSTM.summary()"
   ],
   "id": "C4V6iWDmyjDq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_LSTM.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=[\"mae\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554\n",
    "model_LSTM.fit(X_train[:-VALIDATION_INDEX],\n",
    "            y_train[:-VALIDATION_INDEX],\n",
    "            epochs=10,\n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            validation_data=(X_train[-VALIDATION_INDEX:], y_train[-VALIDATION_INDEX:]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlZrCDyy0yPY",
    "outputId": "38920a54-fc50-4f9d-803b-c55765235684"
   },
   "outputs": [],
   "source": [
    "def make_preds(model, input_data):\n",
    "  \"\"\"\n",
    "  Uses model to make predictions on input_data.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  model: trained model \n",
    "  input_data: windowed input data (same kind of data model was trained on)\n",
    "\n",
    "  Returns model predictions on input_data.\n",
    "  \"\"\"\n",
    "  forecast = model.predict(input_data,verbose=2)\n",
    "  return tf.squeeze(forecast) # return 1D array of predictions\n",
    "\n",
    "predictions = make_preds(model_LSTM, X_test)"
   ],
   "id": "WlZrCDyy0yPY"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if SAVE_RESULTS:\n",
    "    file_path = 'Resources/Resultados/Agregado/'\n",
    "    if LOW_DATA:\n",
    "        file_path += \"LowData/\"\n",
    "    documents = ['minuteY','hourY','dayY']\n",
    "    file = file_path+documents[COMPUTED_OPTION]+\"-predictions\"+\".pkl.gz\"\n",
    "    pickle.dump(predictions, gzip.open(file, 'wb'))\n",
    "    file = file_path+documents[COMPUTED_OPTION]+\"-test\"+\".pkl.gz\"\n",
    "    pickle.dump(y_test, gzip.open(file, 'wb'))\n",
    "    file = file_path+documents[COMPUTED_OPTION]+\"-X\"+\".pkl.gz\"\n",
    "    pickle.dump(X_test, gzip.open(file, 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Resultados comparando directamente las salidas\")\n",
    "if y_test.shape != predictions.shape:\n",
    "    predictions = np.array(predictions).reshape(y_test.shape)\n",
    "print(y_test.shape)\n",
    "print(predictions.shape)\n",
    "print('\\033[1m' + \"MSE: \" + str(mean_squared_error(y_test,predictions)) + '\\033[0m')\n",
    "print('\\033[1m' + \"MAE: \" + str(mean_absolute_error(y_test,predictions)) + '\\033[0m')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not MULTI_STEP_FORECAST:\n",
    "    if SPLIT_INTO_TWO_DAYS:\n",
    "        index = 0\n",
    "        period = X_test[::120,:]\n",
    "        period_results = make_preds(model_LSTM,period)\n",
    "        period_results_to_plot = np.array(period_results).reshape(48)\n",
    "        y_test_to_plot = y_test[::120,:].reshape(48)\n",
    "        plot_predictions_vs_real(predictions=period_results_to_plot,reals=y_test_to_plot)\n",
    "        print('\\033[1m' + \"Predicted: \" + str(np.sum(period_results_to_plot)) + '\\033[0m')\n",
    "        print('\\033[1m' + \"Reals: \" + str(np.sum(y_test_to_plot)) + '\\033[0m')\n",
    "    else:\n",
    "        print(\"Resultados poblacionales cada dos horas\")\n",
    "        print('\\033[1m' + \"MSE: \" + str(mean_squared_error(np.sum(y_test,axis=1),np.sum(predictions,axis=1))) + '\\033[0m')\n",
    "        print('\\033[1m' + \"MAE: \" + str(mean_absolute_error(np.sum(y_test,axis=1),np.sum(predictions,axis=1))) + '\\033[0m')\n",
    "        print(\"Ejemplos aleatorios\")\n",
    "        # Crear un array de índices\n",
    "        indices_totales = np.arange(y_test.shape[0])\n",
    "        # Seleccionar 5 índices aleatorios\n",
    "        indices_aleatorios = np.random.choice(indices_totales, size=5, replace=False)\n",
    "        for i in indices_aleatorios:\n",
    "            if COMPUTED_OPTION == 1:\n",
    "                END = 24\n",
    "                STARTED_MINUTE = 0\n",
    "                previous = np.ones(shape=(24))\n",
    "                for j in range(0,24):\n",
    "                    previous[j] = np.sum(X_test[i,:][60*j:60*(j+1)])\n",
    "            else:\n",
    "                END = 1440\n",
    "                STARTED_MINUTE = 1000\n",
    "                previous = X_test[i,:]\n",
    "\n",
    "            predictions_to_plot = np.ones(shape=(END + PREDICTED_HORIZON))\n",
    "            predictions_to_plot[0:END] = previous[:]\n",
    "            predictions_to_plot[END:] = predictions[i,:]\n",
    "            y_test_to_plot = np.ones(shape=(END + PREDICTED_HORIZON))\n",
    "            y_test_to_plot[0:END] = previous[:]\n",
    "            y_test_to_plot[END:] = y_test[i,:]\n",
    "            plot_predictions_vs_real(predictions_to_plot,y_test_to_plot)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if MULTI_STEP_FORECAST:\n",
    "    day_before = np.array(X_test[0:1,:],dtype=np.float32)\n",
    "    predicted_values_day_one = np.ones(shape=(WINDOW_SIZE,1),dtype=np.float32)\n",
    "    for i in range(0,WINDOW_SIZE):\n",
    "        prediction = float(make_preds(model_LSTM, day_before))\n",
    "        day_before[:,0:-1] = day_before[:,1:]\n",
    "        day_before[:,-1] = prediction\n",
    "        predicted_values_day_one[i,0] = prediction\n",
    "\n",
    "    SECOND_DAY = X_test.shape[1]\n",
    "    day_before = np.array(X_test[SECOND_DAY:SECOND_DAY+1,:],dtype=np.float32)\n",
    "    predicted_values_day_two = np.ones(shape=(WINDOW_SIZE,1),dtype=np.float32)\n",
    "    for i in range(0,WINDOW_SIZE):\n",
    "        prediction = float(make_preds(model_LSTM, day_before))\n",
    "        day_before[:,0:-1] = day_before[:,1:]\n",
    "        day_before[:,-1] = prediction\n",
    "        predicted_values_day_two[i,0] = prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "i_5SRccT8rPJ",
    "outputId": "e996d321-2738-4683-a2f8-b18142a010a1"
   },
   "outputs": [],
   "source": [
    "if MULTI_STEP_FORECAST:\n",
    "    print(\"Predecidos : \" + str(int(np.sum(predicted_values_day_one[:,0]))))\n",
    "    print(\"Reales : \" + str(int(np.sum(y_test[0:WINDOW_SIZE,0]))))\n",
    "    if COMPUTED_OPTION < 2:\n",
    "        plot_predictions_vs_real(predicted_values_day_one[:,0],y_test[0:WINDOW_SIZE,0])"
   ],
   "id": "i_5SRccT8rPJ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if MULTI_STEP_FORECAST:\n",
    "    print(\"Predecidos : \" + str(int(np.sum(predicted_values_day_two[:,0]))))\n",
    "    print(\"Reales : \" + str(int(np.sum(y_test[SECOND_DAY:,0]))))\n",
    "    if COMPUTED_OPTION < 2:\n",
    "        plot_predictions_vs_real(predicted_values_day_two[:,0],y_test[SECOND_DAY:,0])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
